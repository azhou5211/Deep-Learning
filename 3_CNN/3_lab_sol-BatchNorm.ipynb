{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Lab 3: Batch Normalization </h1> </center> \n",
    "<center> Krishna Pillutla, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2022), University of Washington </center>\n",
    "\n",
    "In this lab, we will study batch normalization. \n",
    "For simplicity, we will stick to multilayer perceptrons (MLPs) \n",
    "but the same ideas extend directly to convolutional neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization in linear regression: An optimization viewpoint\n",
    "\n",
    "Recall that we always normalize our features to be zero mean and unit variance \n",
    "(or something similar) on our training set.\n",
    "\n",
    "Let us stack our input data $x_1, \\cdots, x_n \\in \\mathbb{R}^d$ into a matrix $X \\in \\mathbb{R}^{n \\times d}$. Denote the coordinate-wise mean and standard deviation as \n",
    "$$\n",
    "    \\mu^{(j)} = \\frac{1}{n} \\sum_{i=1}^n x_i^{(j)} \\,, \\quad\n",
    "    \\sigma^{(j)} = \\sqrt{\\frac{1}{n} (x_i^{(j)} - \\mu^{(j)})^2} \\,,\n",
    "$$\n",
    "for $j = 1, \\cdots, d$. \n",
    "We can simply write the standardization as\n",
    "$$\n",
    "    \\hat x_i = \\frac{1}{\\sigma}(x_i - \\mu)\n",
    "$$\n",
    "where the operations are applied elementwise. \n",
    "We have applied this transformation to our data in each lab so far. \n",
    "\n",
    "\n",
    "Consider the linear prediction model \n",
    "$\n",
    "f(x) = w^\\top x + b\n",
    "$\n",
    "to predict outputs $y \\approx f(x)$ from inputs $x$. Given $n$ input-output samples $(x_1, y_1), \\ldots, (x_n, y_n) \\in \\mathbb{R}^d \\times \\mathbb{R}$, minimizing the least square loss over the training data reads\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{n}\\sum_{i=1}^n  (y_i - w^\\top x_i - b)^2.\n",
    "$$\n",
    "\n",
    "**Mean Normalization (Homework exercise)**:\n",
    "Show that minimizing in $b$ amounts to center both inputs and outputs leading to \n",
    "$$\n",
    "\\min_{w} f(w) = \\frac{1}{n}\\sum_{i=1}^n  (\\widetilde y_i - w^\\top \\hat x_i)^2.\n",
    "$$\n",
    "where $\\widetilde y$ and $\\hat x$ are respectively the centered outputs and inputs.\n",
    "\n",
    "\n",
    "Recall now that from an optimization point of view, the step-size of a gradient step is controlled by the smoothness of the function, i.e. the Lipschitz-continuity of its gradient, which (for twice continuously differentiable functions) can simply computed as the largest eigenvalue of the Hessian matrix. \n",
    "\n",
    "**Variance Normalization**:\n",
    "Similar to above, one can show that \n",
    "after normalizing the inputs, the objective  $f(w) = \\frac{1}{n}\\sum_{i=1}^n  (\\widetilde y_i - w^\\top \\widehat x_i)^2$ has a smoothness constant no bigger than d.\n",
    "\n",
    "In other words, feature normalization allows one to use a standard learning rate, invariant to the scale of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "We standardize our inputs in neural networks as well, but that only ensures that inputs to the first layer are normalized. What about inputs to subsequent layers?\n",
    "\n",
    "**What is batch norm?**\n",
    "\n",
    "Batch normalization is a technique which generalizes this standardization operation to ensure normalization of inputs in to an intermediate layer. \n",
    "\n",
    "In the linear model case, the mean and variance of our features do not change (since the data is fixed), therefore precomputing them once suffices. \n",
    "However, the inputs to hidden layers in neural networks changes over time. \n",
    "Batch norm, therefore, computes the mean and variance over a *minibatch of data*.\n",
    "\n",
    "The batch norm layer gives the network two *trainable* parameters $\\beta, \\gamma$\n",
    "which are learned over the course of training. If $\\hat x_i$ is the normalized \n",
    "version of input $x_i$ to a batch norm layer, the output of the batch norm layer \n",
    "is \n",
    "$\\gamma \\hat x_i + \\beta$.\n",
    "\n",
    "**Note**: Batch norm requires large minibatch sizes such as 64 or 128 to work.\n",
    "\n",
    "The pseudocode is given below. \n",
    "\n",
    "<img src=\"https://melfm.github.io/posts/2018-08-Understanding-Normalization/img/batch_norm_forward.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch norm in action\n",
    "Our first step is to notice that batch norm behaves differently at train and test times. \n",
    "\n",
    "At test time, there is no reason for a model to receive a large enough minibatch of inputs (think about the deep networks running on your smart phone).\n",
    "Batch norm does not work if the minibatch size is too small. \n",
    "\n",
    "A solution to this issue is to estimate a running mean and standard deviation of the data encountered during training and use this is a proxy at test time. \n",
    "\n",
    "As a result, we need to tell PyTorch to switch from training to evaluation mode and vice-versa using `model.train()` and `model.eval()`, where `model` is a `torch.nn.Module`. \n",
    "\n",
    "**Part 1: Batch norm in action**:\n",
    "Let us visualize how batch norm behaves differently at train and test times on a multilayer perceptron (MLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy, relu\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch's `BatchNorm1d` function. It is called so because the \n",
    "input a batch of vectors in $\\mathbb{R}^d$ (in the form of a matrix of size $n\\times d$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass in input dimensionality (it needs to create mean/std vectors of this dimension)\n",
    "batch_norm_layer = torch.nn.BatchNorm1d(num_features=10)\n",
    "\n",
    "# This is an instance of torch.nn.Module; we can use it as we would a module\n",
    "isinstance(batch_norm_layer, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean:\n",
      "tensor([ 8.7928,  3.0130, 19.6300, 12.3789, 12.1608, 12.4736, 11.2093, 15.9067,\n",
      "         7.7957,  5.1551])\n",
      "Input std:\n",
      "tensor([7.0944, 5.5743, 6.4017, 4.0669, 4.4024, 4.7271, 7.0939, 1.0009, 0.2936,\n",
      "        7.3108])\n"
     ]
    }
   ],
   "source": [
    "# Let us create some synthetic data with a nonzero mean\n",
    "mean_vector = 20 * torch.rand(10)  # non-zero mean\n",
    "std_vector = 10 * torch.rand(10)\n",
    "\n",
    "def get_random_data():\n",
    "    return std_vector[None] * torch.randn(25, 10) + mean_vector[None]  # mean = mean_vector\n",
    "random_data = get_random_data()\n",
    "\n",
    "## Let us go to evaluation mode\n",
    "batch_norm_layer.eval()\n",
    "\n",
    "# print mean and variance of the data\n",
    "print('Input mean:')\n",
    "print(random_data.mean(dim=0))\n",
    "\n",
    "print('Input std:')\n",
    "print(random_data.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mean:\n",
      "tensor([ 8.7928,  3.0130, 19.6299, 12.3789, 12.1607, 12.4736, 11.2092, 15.9067,\n",
      "         7.7957,  5.1551], grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([7.0944, 5.5743, 6.4017, 4.0669, 4.4024, 4.7271, 7.0939, 1.0009, 0.2936,\n",
      "        7.3108], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Let us run our data through the batch norm layer and register its output\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not observe any difference because batch norm in evaluation mode uses \n",
    "historical statistics of mean and std. But since this batch norm layer was just initialized, \n",
    "it has no history to go by.\n",
    "\n",
    "Hence, it uses its defaults: $\\mu = 0$ and $\\sigma = 1$, which makes this batch norm layer just the identity map.\n",
    "\n",
    "Let us switch to the train mode now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mean:\n",
      "tensor([-9.5367e-09,  1.4305e-08, -8.5831e-08,  0.0000e+00, -1.6212e-07,\n",
      "        -8.1062e-08, -6.6757e-08,  2.4796e-07,  3.0994e-07, -1.9073e-08],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206,\n",
      "        1.0206], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "batch_norm_layer.train()  # switch to train mode\n",
    "\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! It works as expected. Let us run a few more random samples and investigate the evaluation mode further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean:\n",
      "tensor([ 6.8452,  4.8642, 20.7527, 14.1600, 14.0118, 12.8186,  9.9461, 15.7668,\n",
      "         7.8273,  4.3325])\n",
      "Input std:\n",
      "tensor([7.8962, 5.4923, 5.0783, 3.0597, 4.5383, 4.3904, 6.0951, 1.1071, 0.3757,\n",
      "        8.3164])\n",
      "Output mean:\n",
      "tensor([-0.2812,  0.0506,  0.3936, -0.1644,  0.2076, -0.0026,  0.0170,  0.0618,\n",
      "        -0.0216, -0.0251], grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([1.1445, 0.8664, 0.7208, 0.6933, 1.0800, 1.0594, 0.9768, 0.9620, 1.1328,\n",
      "        1.2501], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):  # so that batch norm updates its internal statistics\n",
    "    random_data = get_random_data()\n",
    "    output = batch_norm_layer(random_data)\n",
    "    \n",
    "batch_norm_layer.eval()  # switch to eval mode\n",
    "\n",
    "random_data = get_random_data()\n",
    "# print mean and variance of the data\n",
    "print('Input mean:')\n",
    "print(random_data.mean(dim=0))\n",
    "\n",
    "print('Input std:')\n",
    "print(random_data.std(dim=0))\n",
    "\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is much better normalized now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Batch norm + MLPs\n",
    "Just as in the convex case, batch norm helps with learning rates in the nonconvex case as well, especially when the networks are very deep. \n",
    "\n",
    "We start with a simple MLP module. Please read it carefully and understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "                 num_hidden_layers, hidden_width, \n",
    "                 use_batch_norm=False):\n",
    "        ## pass in how many hidden layers to use and width of each layer\n",
    "        ## We will use the same width for all hidden layers\n",
    "        super().__init__()  # call constructor of a super class\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Construct linear layers: use ModuleList to hold a list of submodules\n",
    "        self.hidden_layers = torch.nn.ModuleList([torch.nn.Linear(input_dim, hidden_width)])\n",
    "         # input -> hidden\n",
    "        \n",
    "        for i in range(num_hidden_layers-1):\n",
    "            # hidden i -> hidden i+1\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_width, hidden_width))\n",
    "            \n",
    "        # hidden -> output\n",
    "        self.hidden_to_output = torch.nn.Linear(hidden_width, output_dim)\n",
    "            \n",
    "        # construct batch norm layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        if use_batch_norm:\n",
    "            print('using batch norm')\n",
    "            self.batch_norm_layers = torch.nn.ModuleList(\n",
    "                [torch.nn.BatchNorm1d(hidden_width)\n",
    "                 for _ in range(num_hidden_layers)]\n",
    "            )\n",
    "        else:\n",
    "            print('not using batch norm')\n",
    "            \n",
    "    def forward(self, x,):\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # apply self.hidden_layers[i] to x\n",
    "            x = self.hidden_layers[i](x)\n",
    "            # apply relu to x\n",
    "            x = relu(x)\n",
    "            # apply batch norm if required\n",
    "            if self.use_batch_norm:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "        # hidden -> output\n",
    "        return self.hidden_to_output(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using batch norm\n",
      "MultiLayerPerceptron(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (hidden_to_output): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (batch_norm_layers): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=3, \n",
    "                              use_batch_norm=True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load our FashionMNIST dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the training utilities. \n",
    "\n",
    "**Note**: We must pay extra care to ensure that the model is in training or evaluation mode when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(model, X, y):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        model is a module\n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    # send \n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        ws is a list of tensors of consistent shapes \n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # use eval mode for accuracy\n",
    "    score = model(X)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(model, verbose=False):\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # switch to eval mode\n",
    "    train_loss = compute_objective(model, X_train, y_train)\n",
    "    test_loss = compute_objective(model, X_test, y_test)\n",
    "    train_accuracy = compute_accuracy(model, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy(model, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the minibatch SGD function (copied over from a previous lab). \n",
    "Note again that we must pay special care to making sure the model is in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_sgd_one_pass(model, X, y, learning_rate, batch_size, verbose=False):\n",
    "    model.train()\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    num_updates = int(round(num_examples / batch_size))\n",
    "    for i in range(num_updates):\n",
    "        idxs = np.random.choice(X.shape[0], size=(batch_size,)) # draw `batch_size` many samples\n",
    "        model.train()  # make sure we are in train mode\n",
    "        # compute the objective. \n",
    "        objective = compute_objective(model, X[idxs], y[idxs]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        with torch.no_grad():\n",
    "            for (w, g) in zip(model.parameters(), gradients):\n",
    "                w -= learning_rate * g\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to find the divergent learning rates \n",
    "with and without batch norm. We use a fixed batch size of 32\n",
    "\n",
    "Use a hidden width of 64 throughout. \n",
    "Vary the depth of the network\n",
    "(i.e., the number of hidden layers)\n",
    "between 1 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 2.308, Train Accuracy = 0.105, Test Loss = 2.307, Test Accuracy = 0.100\n",
      "1.4629150733926035\n",
      "1.4603505007998372\n",
      "1.4607902877931949\n",
      "1.4609458159883457\n",
      "1.4599384016740364\n",
      "1.4594744544963225\n",
      "1.4595901984693616\n",
      "1.4595130279157364\n",
      "1.45901046620047\n",
      "1.4599775776629684\n",
      "1.4589347871574259\n",
      "1.4592472592051584\n",
      "1.45975496846663\n",
      "1.4592915087651106\n",
      "1.4590650630614839\n",
      "1.4593566495923778\n",
      "1.459366981973466\n",
      "1.4599395390315637\n",
      "1.4596220073021942\n",
      "1.4597148669864586\n",
      "1.4597667099670277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-426636d63aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# run multiple passes because our sub-sampled dataset is too small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n\u001b[0;32m---> 19\u001b[0;31m                                    batch_size=batch_size, verbose=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-5b44d9ab85cc>\u001b[0m in \u001b[0;36mminibatch_sgd_one_pass\u001b[0;34m(model, X, y, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# draw `batch_size` many samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make sure we are in train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# compute the objective.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/data598/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/data598/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/data598/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \"\"\"\n\u001b[1;32m   1275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/data598/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mchildren\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mchild\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \"\"\"\n\u001b[0;32m-> 1172\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/data598/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         r\"\"\"Returns an iterator over immediate children modules, yielding both\n\u001b[1;32m   1177\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwell\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 25\n",
    "\n",
    "# no batch norm\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=num_hidden_layers, \n",
    "                              use_batch_norm=False)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "logs = []\n",
    "\n",
    "logs.append(compute_logs(model, verbose=True))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(batch_size*2):  # run multiple passes because our sub-sampled dataset is too small\n",
    "    model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                   batch_size=batch_size, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using batch norm\n",
      "Train Loss = 2.305, Train Accuracy = 0.100, Test Loss = 2.305, Test Accuracy = 0.100\n",
      "1.4305942569650365\n",
      "1.30654127367001\n",
      "1.1369090320665378\n",
      "1.0542684557144164\n",
      "1.015251660206855\n",
      "0.9947519833189675\n",
      "1.0859425495061046\n",
      "1.0456342753880443\n",
      "1.0263897274705627\n",
      "0.9966497322805704\n",
      "1.001491148548971\n",
      "0.9413835397390786\n",
      "0.9006339604277721\n",
      "0.9041905662616612\n",
      "0.9076953421547508\n",
      "0.8721404288450858\n",
      "0.8857889673731856\n",
      "0.8959513828010595\n",
      "0.8894616399102827\n",
      "0.8294174185502105\n",
      "0.8236112029959983\n",
      "0.8484262439533613\n",
      "0.8242413214436606\n",
      "0.8400735810110564\n",
      "0.8058097218654495\n",
      "0.8159454856198688\n",
      "0.7777750871295128\n",
      "0.7873107574194917\n",
      "0.7764817834693138\n",
      "0.7543891809173369\n",
      "0.8028543857458957\n",
      "0.7739856039789885\n"
     ]
    }
   ],
   "source": [
    "# with batch norm\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=num_hidden_layers, \n",
    "                              use_batch_norm=True)\n",
    "\n",
    "\n",
    "learning_rate = 1.28\n",
    "\n",
    "logs = []\n",
    "\n",
    "logs.append(compute_logs(model, verbose=True))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(batch_size):  # run multiple passes because our sub-sampled dataset is too small\n",
    "    model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                   batch_size=batch_size, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the last part with depths of $10$ and $25$, and $50$. What do you observe?\n",
    "\n",
    "**NOTE**: the larger models (depth > 20) get very slow. Use a cloud computing resource if it is too slow on your laptop.\n",
    "\n",
    "**Hint**: If some of the models at these depths get stuck or suddenly diverge to infinity/NaN, know that it is an often encountered problem with very deep networks. A combination of many factors can overcome this issue: careful initialization and tuning of learning rates as well as dropout. The most popular fix, however, is to use residual networks, know as _ResNets_. We will talk about these in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
