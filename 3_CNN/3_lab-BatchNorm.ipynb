{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Lab 3: Batch Normalization </h1> </center> \n",
    "<center> Krishna Pillutla, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2022), University of Washington </center>\n",
    "\n",
    "In this lab, we will study batch normalization. \n",
    "For simplicity, we will stick to multilayer perceptrons (MLPs) \n",
    "but the same ideas extend directly to convolutional neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution review\n",
    "Before we start, we quickly look at convolutions in practice. We will look at the input size, input channels, output channels, etc.\n",
    "\n",
    "The arguments are:\n",
    "\n",
    "- in_channels: number of channels in the input image\n",
    "- out_channels: number of channels in the output image. Equivalently, the number of filters\n",
    "- kernel size: the size of kernel/filter. E.g. $3 \\times 3$ or $5 \\times 5$\n",
    "- padding: how much zero padding to perform. Everything beyond the padding is dropped\n",
    "- stride: the stride of the convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "convolution_layer = torch.nn.Conv2d(\n",
    "    in_channels=3, out_channels=16, kernel_size=5,\n",
    "    padding=3, stride=3\n",
    ")\n",
    "# this is an instance of `torch.nn.Module`\n",
    "print(isinstance(convolution_layer, torch.nn.Module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 76, 76])\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "random_images = torch.randn(1, 3, image_size, image_size) \n",
    "# 1 is the batch dimension; 3 is the number of channles\n",
    "# batch dimension: the convolution operation is applied in parallel for element of the batch\n",
    "\n",
    "output = convolution_layer(random_images)\n",
    "## Exercise: parse the shape of the output\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 570, 570])\n"
     ]
    }
   ],
   "source": [
    "image_size = 574\n",
    "random_images = torch.randn(8, 3, image_size, image_size)  \n",
    "# 8 is the batch dimension; 3 is the number of channels\n",
    "\n",
    "output = convolution_layer(random_images)\n",
    "## Exercise: parse the shape of the output\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the convolution layer only requires the number of input and output channels. \n",
    "The size of the image or the size of the batch does not matter! Why do you think this is the case?\n",
    "\n",
    "Ans:\n",
    "This is because the kernel will be applied to whole image size (and rest will be padded) and batch does not matter since it has to be applied to all the images\n",
    "\n",
    "**Exercises**: \n",
    "- Play with the image sizes above to understand the outputs you get\n",
    "- Change the padding and the stride. See if you can figure out a formula between how the output size relates to the input and kernel sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter names: dict_keys(['weight', 'bias'])\n",
      "torch.Size([16, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "parameters = dict(convolution_layer.named_parameters())\n",
    "print('parameter names:', parameters.keys())\n",
    "\n",
    "print(parameters['weight'].shape)  # what does the shape denote?\n",
    "\n",
    "#torch.Size([16, 3, 5, 5])\n",
    "#5 - kernel size\n",
    "#3 - number of input channels\n",
    "#16 - number of output channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization in linear regression: An optimization viewpoint\n",
    "\n",
    "Recall that we always normalize our features to be zero mean and unit variance \n",
    "(or something similar) on our training set.\n",
    "\n",
    "Let us stack our input data $x_1, \\cdots, x_n \\in \\mathbb{R}^d$ into a matrix $X \\in \\mathbb{R}^{n \\times d}$. Denote the coordinate-wise mean and standard deviation as \n",
    "$$\n",
    "    \\mu^{(j)} = \\frac{1}{n} \\sum_{i=1}^n x_i^{(j)} \\,, \\quad\n",
    "    \\sigma^{(j)} = \\sqrt{\\frac{1}{n} (x_i^{(j)} - \\mu^{(j)})^2} \\,,\n",
    "$$\n",
    "for $j = 1, \\cdots, d$. \n",
    "We can simply write the standardization as\n",
    "$$\n",
    "    \\hat x_i = \\frac{1}{\\sigma}(x_i - \\mu)\n",
    "$$\n",
    "where the operations are applied elementwise. \n",
    "We have applied this transformation to our data in each lab so far. \n",
    "\n",
    "\n",
    "Consider the linear prediction model \n",
    "$\n",
    "f(x) = w^\\top x + b\n",
    "$\n",
    "to predict outputs $y \\approx f(x)$ from inputs $x$. Given $n$ input-output samples $(x_1, y_1), \\ldots, (x_n, y_n) \\in \\mathbb{R}^d \\times \\mathbb{R}$, minimizing the least square loss over the training data reads\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{n}\\sum_{i=1}^n  (y_i - w^\\top x_i - b)^2.\n",
    "$$\n",
    "\n",
    "**Mean Normalization (Homework exercise)**:\n",
    "Show that minimizing in $b$ amounts to center both inputs and outputs leading to \n",
    "$$\n",
    "\\min_{w} f(w) = \\frac{1}{n}\\sum_{i=1}^n  (\\widetilde y_i - w^\\top \\hat x_i)^2.\n",
    "$$\n",
    "where $\\widetilde y$ and $\\hat x$ are respectively the centered outputs and inputs.\n",
    "\n",
    "\n",
    "Recall now that from an optimization point of view, the step-size of a gradient step is controlled by the smoothness of the function, i.e. the Lipschitz-continuity of its gradient, which (for twice continuously differentiable functions) can simply computed as the largest eigenvalue of the Hessian matrix. \n",
    "\n",
    "**Variance Normalization**:\n",
    "Similar to above, one can show that \n",
    "after normalizing the inputs, the objective  $f(w) = \\frac{1}{n}\\sum_{i=1}^n  (\\widetilde y_i - w^\\top \\widehat x_i)^2$ has a smoothness constant no bigger than d.\n",
    "\n",
    "In other words, feature normalization allows one to use a standard learning rate, invariant to the scale of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "We standardize our inputs in neural networks as well, but that only ensures that inputs to the first layer are normalized. What about inputs to subsequent layers?\n",
    "\n",
    "**What is batch norm?**\n",
    "\n",
    "Batch normalization is a technique which generalizes this standardization operation to ensure normalization of inputs in to an intermediate layer. \n",
    "\n",
    "In the linear model case, the mean and variance of our features do not change (since the data is fixed), therefore precomputing them once suffices. \n",
    "However, the inputs to hidden layers in neural networks changes over time. \n",
    "Batch norm, therefore, computes the mean and variance over a *minibatch of data*.\n",
    "\n",
    "The batch norm layer gives the network two *trainable* parameters $\\beta, \\gamma$\n",
    "which are learned over the course of training. If $\\hat x_i$ is the normalized \n",
    "version of input $x_i$ to a batch norm layer, the output of the batch norm layer \n",
    "is \n",
    "$\\gamma \\hat x_i + \\beta$.\n",
    "\n",
    "**Note**: Batch norm requires large minibatch sizes such as 64 or 128 to work.\n",
    "\n",
    "The pseudocode is given below. \n",
    "\n",
    "<img src=\"https://melfm.github.io/posts/2018-08-Understanding-Normalization/img/batch_norm_forward.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch norm in action\n",
    "Our first step is to notice that batch norm behaves differently at train and test times. \n",
    "\n",
    "At test time, there is no reason for a model to receive a large enough minibatch of inputs (think about the deep networks running on your smart phone).\n",
    "Batch norm does not work if the minibatch size is too small. \n",
    "\n",
    "A solution to this issue is to estimate a running mean and standard deviation of the data encountered during training and use this is a proxy at test time. \n",
    "\n",
    "As a result, we need to tell PyTorch to switch from training to evaluation mode and vice-versa using `model.train()` and `model.eval()`, where `model` is a `torch.nn.Module`. \n",
    "\n",
    "## Part 1: Batch norm in action\n",
    "Let us visualize how batch norm behaves differently at train and test times on a multilayer perceptron (MLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy, relu\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch's `BatchNorm1d` function. It is called so because the \n",
    "input a batch of vectors in $\\mathbb{R}^d$ (in the form of a matrix of size $n\\times d$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass in input dimensionality (it needs to create mean/std vectors of this dimension)\n",
    "batch_norm_layer = torch.nn.BatchNorm1d(num_features=10)\n",
    "\n",
    "# This is an instance of torch.nn.Module; we can use it as we would a module\n",
    "isinstance(batch_norm_layer, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean:\n",
      "tensor([ 1.2290,  8.2977, 17.4594,  6.0496,  5.0881,  4.7807,  9.4291,  1.7779,\n",
      "         1.1503, 19.8451])\n",
      "Input std:\n",
      "tensor([2.5954, 3.4691, 5.9597, 0.4528, 3.8345, 5.8139, 3.2988, 4.0678, 5.6456,\n",
      "        0.0209])\n"
     ]
    }
   ],
   "source": [
    "# Let us create some synthetic data with a nonzero mean\n",
    "mean_vector = 20 * torch.rand(10)  # non-zero mean\n",
    "std_vector = 10 * torch.rand(10)\n",
    "\n",
    "def get_random_data():\n",
    "    return std_vector[None] * torch.randn(25, 10) + mean_vector[None]  # mean = mean_vector\n",
    "random_data = get_random_data()\n",
    "\n",
    "## Let us go to evaluation mode\n",
    "batch_norm_layer.eval()\n",
    "\n",
    "# print mean and variance of the data\n",
    "print('Input mean:')\n",
    "print(random_data.mean(dim=0))\n",
    "\n",
    "print('Input std:')\n",
    "print(random_data.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mean:\n",
      "tensor([ 1.2290,  8.2976, 17.4593,  6.0496,  5.0881,  4.7807,  9.4291,  1.7779,\n",
      "         1.1503, 19.8450], grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([2.5954, 3.4691, 5.9596, 0.4528, 3.8345, 5.8139, 3.2988, 4.0677, 5.6455,\n",
      "        0.0209], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let us run our data through the batch norm layer and register its output\n",
    "output = batch_norm_layer(random_data) # <TODO: your code here> apply `batch_norm_layer` to `random_data`\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not observe any difference because batch norm in evaluation mode uses \n",
    "historical statistics of mean and std. But since this batch norm layer was just initialized, \n",
    "it has no history to go by.\n",
    "\n",
    "Hence, it uses its defaults: $\\mu = 0$ and $\\sigma = 1$, which makes this batch norm layer just the identity map.\n",
    "\n",
    "Let us switch to the train mode now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mean:\n",
      "tensor([ 2.8610e-08,  1.2398e-07,  4.2915e-08, -4.4823e-07, -2.8610e-08,\n",
      "         0.0000e+00, -7.8678e-08,  1.1921e-08, -1.4305e-08,  4.3726e-05],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206,\n",
      "        1.0086], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_norm_layer.train()  # switch to train mode\n",
    "\n",
    "output = batch_norm_layer(random_data)# <TODO: your code here> apply `batch_norm_layer` to `random_data`\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! It works as expected. Let us run a few more random samples and investigate the evaluation mode further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean:\n",
      "tensor([ 2.4337, 11.4200, 17.0795,  6.0232,  2.9947,  5.7541,  9.0437,  1.9582,\n",
      "         1.9884, 19.8391])\n",
      "Input std:\n",
      "tensor([3.0015, 3.7518, 3.2199, 0.5087, 4.4398, 4.9793, 3.1715, 3.4411, 4.6568,\n",
      "        0.0298])\n",
      "Output mean:\n",
      "tensor([ 0.1999,  0.2792,  0.0688,  0.1627, -0.4313,  0.0950, -0.0233,  0.2644,\n",
      "         0.1290, -0.1525], grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([1.1000, 0.9450, 0.8250, 0.8673, 1.0863, 0.7997, 1.0950, 0.7424, 0.9724,\n",
      "        1.3195], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):  # so that batch norm updates its internal statistics\n",
    "    random_data = get_random_data()\n",
    "    output = batch_norm_layer(random_data)\n",
    "    \n",
    "batch_norm_layer.eval()  # switch to eval mode\n",
    "\n",
    "random_data = get_random_data()\n",
    "# print mean and variance of the data\n",
    "print('Input mean:')\n",
    "print(random_data.mean(dim=0))\n",
    "\n",
    "print('Input std:')\n",
    "print(random_data.std(dim=0))\n",
    "\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is much better normalized now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Batch norm + MLPs\n",
    "Just as in the convex case, batch norm helps with learning rates in the nonconvex case as well, especially when the networks are very deep. \n",
    "\n",
    "We start with a simple MLP module. Please read it carefully and understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "                 num_hidden_layers, hidden_width, \n",
    "                 use_batch_norm=False):\n",
    "        ## pass in how many hidden layers to use and width of each layer\n",
    "        ## We will use the same width for all hidden layers\n",
    "        super().__init__()  # call constructor of a super class\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Construct linear layers: use ModuleList to hold a list of submodules\n",
    "        self.hidden_layers = torch.nn.ModuleList([torch.nn.Linear(input_dim, hidden_width)])\n",
    "         # input -> hidden\n",
    "        \n",
    "        for i in range(num_hidden_layers-1):\n",
    "            # hidden i -> hidden i+1\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_width, hidden_width))\n",
    "            \n",
    "        # hidden -> output\n",
    "        self.hidden_to_output = torch.nn.Linear(hidden_width, output_dim)\n",
    "            \n",
    "        # construct batch norm layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        if use_batch_norm:\n",
    "            print('using batch norm')\n",
    "            self.batch_norm_layers = torch.nn.ModuleList(\n",
    "                [torch.nn.BatchNorm1d(hidden_width)\n",
    "                 for _ in range(num_hidden_layers)]\n",
    "            )\n",
    "        else:\n",
    "            print('not using batch norm')\n",
    "            \n",
    "    def forward(self, x,):\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # apply self.hidden_layers[i] to x\n",
    "            x = # <TODO: your code here> \n",
    "            # apply the `relu` function to x\n",
    "            x = # <TODO: your code here> \n",
    "            # apply batch norm if required\n",
    "            if self.use_batch_norm:\n",
    "                # apply `batch_norm_layers[i]` to x\n",
    "                x = # <TODO: your code here> \n",
    "        # hidden -> output\n",
    "        # apply `hidden_to_output` to x\n",
    "        x = # <TODO: your code here> \n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=3, \n",
    "                              use_batch_norm=True)\n",
    "\n",
    "print(model) # Do you see all the weights and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load our FashionMNIST dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the training utilities. \n",
    "\n",
    "**Note**: We must pay extra care to ensure that the model is in training or evaluation mode when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(model, X, y):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        model is a module\n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    # TODO: your code here\n",
    "    # compute the `cross_entropy` loss between `model` applied to `X` and `y`\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        ws is a list of tensors of consistent shapes \n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # use eval mode for accuracy\n",
    "    score = model(X)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(model, verbose=False):\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # switch to eval mode\n",
    "    train_loss = compute_objective(model, X_train, y_train)\n",
    "    test_loss = compute_objective(model, X_test, y_test)\n",
    "    train_accuracy = compute_accuracy(model, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy(model, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the minibatch SGD function (copied over from a previous lab). \n",
    "Note again that we must pay special care to making sure the model is in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_sgd_one_pass(model, X, y, learning_rate, batch_size, verbose=False):\n",
    "    model.train()\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    num_updates = int(round(num_examples / batch_size))\n",
    "    for i in range(num_updates):\n",
    "        idxs = np.random.choice(X.shape[0], size=(batch_size,)) # draw `batch_size` many samples\n",
    "        model.train()  # make sure we are in train mode\n",
    "        # compute the objective. \n",
    "        objective = # <TODO: your code here> \n",
    "        \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient and make SGD update with learning rate\n",
    "        # <TODO: your code here> \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to find the divergent learning rates \n",
    "with and without batch norm. We use a fixed batch size of 32\n",
    "\n",
    "Use a hidden width of 64 throughout. \n",
    "Vary the depth of the network\n",
    "(i.e., the number of hidden layers)\n",
    "between 1 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 1\n",
    "\n",
    "# no batch norm\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=num_hidden_layers, \n",
    "                              use_batch_norm=False)\n",
    "\n",
    "\n",
    "learning_rate = # <TODO: your values here> \n",
    "\n",
    "logs = []\n",
    "\n",
    "logs.append(compute_logs(model, verbose=True))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(batch_size*2):  # run multiple passes because our sub-sampled dataset is too small\n",
    "    model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                   batch_size=batch_size, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with batch norm\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=num_hidden_layers, \n",
    "                              use_batch_norm=True)\n",
    "\n",
    "\n",
    "learning_rate = # <TODO: your values here> \n",
    "\n",
    "logs = []\n",
    "\n",
    "logs.append(compute_logs(model, verbose=True))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(batch_size):  # run multiple passes because our sub-sampled dataset is too small\n",
    "    model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                   batch_size=batch_size, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the last part with depths of $10$ and $25$, and $50$. What do you observe?\n",
    "\n",
    "**NOTE**: the larger models (depth > 20) get very slow. Use a cloud computing resource if it is too slow on your laptop.\n",
    "\n",
    "**Hint**: If some of the models at these depths get stuck or suddenly diverge to infinity/NaN, know that it is an often encountered problem with very deep networks. A combination of many factors can overcome this issue: careful initialization and tuning of learning rates as well as dropout. The most popular fix, however, is to use residual networks, know as _ResNets_. We will talk about these in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
