{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03eb6b7",
   "metadata": {},
   "source": [
    "# \n",
    "<center><h1> Homework 3: ConvNet\n",
    "<center> Apoorv Sharma </center>\n",
    "    <center> DATA 598 (Winter 2022), University of Washington </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ac36e",
   "metadata": {},
   "source": [
    "# 1. The Effect of BatchNorm on a ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c44d5f",
   "metadata": {},
   "source": [
    "In this exercise, we will combine both the topics we covered in class this week. The goal of this exercise is to\n",
    "visualize the effective smoothness of a covolutional neural network with and without batch normalization.\n",
    "\n",
    "Let $\\phi(.; \\omega) : \\mathbb{R}^{28x28} \\rightarrow \\mathbb{R}^{10}$ denote a convolution neural network with parameters $\\omega$ which takes in an image of size $28 × 28$ and returns a score for 10 output classes (All the MLPs and ConvNets we have considered so far fit this input-output description of $\\phi$, upto a reshaping of the images). Consider the\n",
    "objective function:\n",
    "\n",
    "$$\n",
    "f(\\omega) = \\frac{1}{n} \\sum_{i=1}^{n} l(y_i, \\phi(x_i,\\omega))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec1a923",
   "metadata": {},
   "source": [
    "Concretely, your task is as follows:\n",
    "* Use the FashionMNIST dataset. Perform the same preprocessing as in previous homeworks.\n",
    "* Code up a ConvNet module with two convolutional layers with the following structure (the input has 1 channel, so we write the image as 1 × 28 × 28):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "60e9d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy, relu\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6ae5693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './models'\n",
    "if not os.path.exists(path):\n",
    "  os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ce2ad",
   "metadata": {},
   "source": [
    "#### CovNet1 Specification\n",
    "\n",
    "* k denotes the kernel/filter size and “#filters” denotes the number of filters\n",
    "* In PyTorch, the convolutions and pooling operations on images are called “Conv2d” and “Max-Pool2d” respectively.\n",
    "* For the first conv layer, the specification asks you to use a kernel size of 5, and a padding of 2. The number of input channels is the same as the number of channels from the preceding layer (here, it is 1 since the preceding layer is just the image with 1 channel). Finally, the number of output channels is the same as the number of filters (here, 16). The second conv layer is constructed in a similarly; the number of input channels is the same as the number of outputs channels of the first conv layer (since ReLU and MaxPool do not change the number of channels). When not specified, we take the stride to be 1.\n",
    "* The last “Linear” layer takes in the output of the second MaxPool and flattens it down to a vector of a certain size S. You are to figure out this size by running a dummy input through these layers and analyzing the output size, as we have done in the lab. The linear layer then maps this S-dimensional input to a 10-dimensional output, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "20c5a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovNetNoBatchNorm(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=1))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=1))\n",
    "        \n",
    "        # cov_ensemble_2 has shape: torch.Size([1, 32, 26, 26])\n",
    "        self.fully_connected_layer = torch.nn.Linear(26*26*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)  # reshape input; convolutions need a channel\n",
    "        out = self.conv_ensemble_1(x)  # first convolution + relu + pooling\n",
    "        out = self.conv_ensemble_2(out) # second convolution + relu + pooling\n",
    "        out = out.view(out.shape[0], -1)  # flatten output\n",
    "        out = self.fully_connected_layer(out)  # output layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "91163150",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "random_image = torch.randn(1, 1, image_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "36cfd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovNetBatchNorm(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=1),\n",
    "            torch.nn.BatchNorm2d(16))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=1),\n",
    "            torch.nn.BatchNorm2d(32))\n",
    "        \n",
    "        # cov_ensemble_2 has shape: torch.Size([1, 32, 26, 26])\n",
    "        self.fully_connected_layer = torch.nn.Linear(26*26*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)  # reshape input; convolutions need a channel\n",
    "        out = self.conv_ensemble_1(x)  # first convolution + relu + pooling\n",
    "        out = self.conv_ensemble_2(out) # second convolution + relu + pooling\n",
    "        out = out.view(out.shape[0], -1)  # flatten output\n",
    "        out = self.fully_connected_layer(out)  # output layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9e177",
   "metadata": {},
   "source": [
    "Test the models and ensure that they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6aa225b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = CovNetNoBatchNorm(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "654e803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = CovNetBatchNorm(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a18ec5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0470, -0.0932,  0.0651,  0.3884,  0.3967, -0.1737, -0.2109, -0.2891,\n",
      "          0.0209, -0.1509]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = m1(random_image)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9b4ed308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5358, -0.4206, -0.7428, -0.3545,  0.7004, -0.1301, -1.1664, -0.8595,\n",
      "          0.3647,  0.7489]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = m2(random_image)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c61cc2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "127dcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(model, X, y):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        model is a module\n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    # send \n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        ws is a list of tensors of consistent shapes \n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # use eval mode for accuracy\n",
    "    score = model(X)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(model, verbose=False):\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # switch to eval mode\n",
    "    train_loss = compute_objective(model, X_train, y_train)\n",
    "    test_loss = compute_objective(model, X_test, y_test)\n",
    "    train_accuracy = compute_accuracy(model, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy(model, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ad7f19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_sgd_one_pass(model, X, y, learning_rate, batch_size, verbose=False):\n",
    "    model.train()\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    num_updates = int(round(num_examples / batch_size))\n",
    "    L_hats = []\n",
    "    for i in range(num_updates):\n",
    "        idxs = np.random.choice(X.shape[0], size=(batch_size,)) # draw `batch_size` many samples\n",
    "        model.train()  # make sure we are in train mode\n",
    "        # compute the objective. \n",
    "        objective = compute_objective(model, X[idxs], y[idxs]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # Get the current objective\n",
    "        model.eval()\n",
    "        w_old = model.parameters()\n",
    "        delta_f_w = torch.autograd.grad(outputs=compute_objective(model, X, y) , inputs=w_old)\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        with torch.no_grad():\n",
    "            for (w, g) in zip(model.parameters(), gradients):\n",
    "                w -= learning_rate * g\n",
    "        \n",
    "        u = model.parameters() - w_old\n",
    "        delta_f_uw = torch.autograd.grad(outputs=compute_objective(model, X, y) , inputs=model.parameters())\n",
    "        \n",
    "        L_hat = 0\n",
    "        for old, new in zip(delta_f_w, delta_f_uw):\n",
    "            L_hat += torch.linalg.norm((new - old), ord=2) / torch.linalg.norm(u, ord=2)\n",
    "        \n",
    "        L_hats.append(L_hat)\n",
    "        \n",
    "    return model, L_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a5de17c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 2.304, Train Accuracy = 0.114, Test Loss = 2.306, Test Accuracy = 0.119\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'generator' and 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6449/1878322728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_passes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         model, L_hat = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                        batch_size=batch_size, verbose=True)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6449/1873818813.py\u001b[0m in \u001b[0;36mminibatch_sgd_one_pass\u001b[0;34m(model, X, y, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdelta_f_uw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'generator' and 'generator'"
     ]
    }
   ],
   "source": [
    "batch_logs = []\n",
    "learning_rate = 0.01\n",
    "num_passes = 10\n",
    "batch_size = 32\n",
    "\n",
    "m1 = CovNetNoBatchNorm(num_classes=10)\n",
    "m2 = CovNetBatchNorm(num_classes=10)\n",
    "\n",
    "for i, model in enumerate([m1, m2]):\n",
    "    logs = []\n",
    "    logs.append( list(compute_logs(model, verbose=True)) + [0] )\n",
    "    \n",
    "    for _ in range(num_passes):\n",
    "        model, L_hat = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                       batch_size=batch_size, verbose=True)\n",
    "        \n",
    "        log = list(compute_logs(model, True)) + [L_hat]\n",
    "        logs.append(log)\n",
    "    \n",
    "    # done training this mode - append logs\n",
    "    batch_logs.append(logs)\n",
    "    \n",
    "    # save the model parms\n",
    "    torch.save(model.state_dict(), f'./models/{type(model).__name__}.pt')\n",
    "\n",
    "with open('./models/logs.pkl', 'wb') as f:\n",
    "    pickle.dump(batch_logs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9570af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = np.random.choice(X_train.shape[0], size=(batch_size,)) # draw `batch_size` many samples\n",
    "# objective = compute_objective(m1, X_train[idxs], y_train[idxs])\n",
    "# print(objective)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
